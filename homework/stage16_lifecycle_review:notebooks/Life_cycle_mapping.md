# Applied Financial Engineering — Framework Guide: Portfolio Optimization Project

This Framework Guide documents my portfolio optimization project that simulates retail investor behavior through quantitative stock selection using 14 years of historical data (2010-2024).

---

## Framework Guide Table

| Lifecycle Stage | What You Did | Challenges | Solutions / Decisions | Future Improvements |
|-----------------|--------------|------------|-----------------------|---------------------|
| **1. Problem Framing & Scoping** | Defined the problem as mimicking retail investor behavior: building concentrated portfolios (8-15 stocks, 3 stocks >50% weight) to identify top-performing stocks through 1,000 portfolio simulations. Success metrics: CAGR >18%, Sharpe ratio >1.5, max drawdown <25%. | Balancing academic rigor with realistic retail constraints was challenging. Deciding whether to optimize mathematically or simulate human behavior created scope ambiguity. | Chose simulation approach over optimization to better reflect real investor patterns. Defined clear constraints (portfolio size, concentration rules) and performance thresholds based on market benchmarks. | Would conduct stakeholder interviews with actual retail investors to validate behavioral assumptions. Could expand to include transaction costs and behavioral biases. |
| **2. Tooling Setup** | Configured Python environment with pandas, numpy, matplotlib for data analysis. Set up Kaggle API for data access. Used Jupyter notebooks for iterative development and visualization creation. | Initial dependency conflicts between numpy versions and matplotlib rendering issues in different environments. Kaggle API authentication setup was non-trivial. | Created virtual environment with pinned package versions. Documented exact dependency requirements. Set up proper Kaggle credentials and tested API connectivity early. | Would containerize the environment using Docker for better reproducibility. Automate environment setup with requirements.txt and setup scripts. |
| **3. Python Fundamentals** | Applied core pandas for data manipulation (14 years × multiple stocks), numpy for mathematical calculations (CAGR, volatility, Sharpe ratios), and matplotlib for bubble charts and visualizations. Used list comprehensions and vectorized operations for performance. | Memory management with large datasets (2010-2024) caused some performance issues. Complex nested loops for portfolio simulation initially ran slowly. | Optimized using pandas vectorized operations instead of loops. Implemented data chunking for memory management. Cached intermediate calculations to avoid recomputation. | Would strengthen skills in parallel processing (multiprocessing) for portfolio simulations. Learn more advanced pandas optimization techniques. |
| **4. Data Acquisition / Ingestion** | Sourced historical stock price data from Kaggle API covering 2010-2024. Ingested daily price data for Indian stock market (NSE/BSE). Structured ingestion to handle multiple years and stocks systematically. | API rate limits occasionally caused timeouts. Some stocks had incomplete historical data or were delisted during the period. Data format inconsistencies across different time periods. | Implemented retry logic with exponential backoff for API calls. Built validation checks to ensure data completeness. Created fallback mechanisms for missing data periods. | Would implement automated daily data updates. Add multiple data sources for redundancy (Yahoo Finance, Alpha Vantage). Build more robust data validation pipelines. |
| **5. Data Storage** | Stored data in CSV format organized by date ranges. Used pandas DataFrames as primary in-memory structure. Maintained separate files for raw data and processed results. | CSV files became large (14 years of daily data). No database optimization led to slower read/write operations. File versioning became challenging. | Organized data in logical folder structure (raw/, processed/, results/). Implemented consistent naming conventions with timestamps. Added data compression for storage efficiency. | Would migrate to time-series database (InfluxDB) or columnar storage (Parquet). Implement proper data versioning and backup strategies. |
| **6. Data Preprocessing** | Cleaned missing values, handled stock splits and dividends, normalized price data for different stocks. Filtered out illiquid stocks and those with insufficient history. Standardized date formats and ensured consistent time series. | Corporate actions (splits, mergers) created data discontinuities. Some stocks had significant data gaps. Different stocks had different listing dates creating uneven time series. | Implemented forward-fill for minor gaps, dropped stocks with >10% missing data. Adjusted for stock splits using corporate action data. Created standardized date index for all stocks. | Would implement more sophisticated corporate action adjustments. Add sector/industry classifications for better analysis. Build automated data quality monitoring. |
| **7. Outlier Analysis** | Detected extreme price movements (>15% daily changes) and investigated whether they were data errors or real market events. Analyzed stocks with unusual volatility patterns during market crashes (2020). | Distinguishing between data errors and legitimate market events (like COVID crash) was difficult. Some outliers were valid but extreme market reactions. | Cross-validated suspicious data points against external sources. Kept outliers that could be verified as real market events. Documented all outlier decisions for transparency. | Would implement statistical outlier detection (Z-score, IQR methods). Build automated outlier flagging system. Add news sentiment analysis to contextualize extreme movements. |
| **8. Exploratory Data Analysis (EDA)** | Created correlation heatmaps, return distributions, volatility analysis across time periods. Visualized portfolio performance distributions and risk-return scatter plots. Analyzed sector concentration and stock frequency patterns. | Initial correlation analysis showed most stocks were moderately correlated (avg 0.23), making diversification challenging. Some visualizations were cluttered with too many data points. | Focused EDA on key metrics (CAGR, Sharpe, drawdown). Created interactive plots for better exploration. Segmented analysis by time periods and market conditions. | Would add sector-wise analysis and factor decomposition. Implement rolling correlation analysis. Create automated EDA reports for stakeholder consumption. |
| **9. Feature Engineering** | Calculated financial metrics: CAGR, annualized volatility, Sharpe ratio, maximum drawdown. Created correlation matrices for diversification analysis. Engineered scoring system (Frequency × Weight) for stock ranking. | Defining appropriate risk-free rate for Sharpe calculation was challenging. Some metrics were sensitive to time period selection. Portfolio scoring methodology required multiple iterations. | Used 10-year government bond yield as risk-free rate. Standardized all calculations to annual basis. Validated scoring methodology against known good/bad performers. | Would add technical indicators (RSI, moving averages). Create regime-dependent metrics. Implement factor-based risk models (Fama-French). |
| **10. Modeling (Regression / Time Series / Classification)** | Built portfolio simulation model generating 1,000 random portfolios with retail constraints. Used Monte Carlo approach for portfolio construction. Applied ranking algorithm based on risk-adjusted performance metrics. | Random portfolio generation sometimes created unrealistic allocations. Model didn't account for transaction costs or liquidity constraints. Ensuring reproducibility across simulation runs was challenging. | Implemented validation checks for portfolio constraints. Added random seed for reproducible results. Created bounds checking for allocation reasonableness. | Would implement mean-variance optimization as comparison baseline. Add regime-switching models for different market conditions. Include transaction cost modeling. |
| **11. Evaluation & Risk Communication** | Evaluated portfolios using multiple metrics: CAGR (18.5% target), Sharpe ratio (1.85 average for top 5), maximum drawdown analysis. Communicated concentration risk and correlation assumptions clearly. | Stakeholders had difficulty understanding why concentrated portfolios could be optimal. Risk metrics sometimes contradicted each other (high return vs high volatility). | Created clear visualizations showing risk-return tradeoffs. Explained retail investor context and behavioral rationale. Provided multiple risk perspectives (volatility, drawdown, correlation). | Would add stress testing under different market scenarios. Implement Value-at-Risk calculations. Create risk attribution analysis by stock/sector. |
| **12. Results Reporting, Delivery Design & Stakeholder Communication** | Delivered top 10 stock recommendations with supporting analysis. Created bubble charts showing frequency vs weight relationships. Produced executive summary highlighting key findings (CHOICEIN as top pick). | Non-technical stakeholders struggled with statistical concepts like Sharpe ratio. Visual complexity in bubble charts initially confused audiences. | Simplified language and focused on practical implications. Added color-coding and legends to improve chart readability. Created tiered reporting (executive summary, technical details). | Would develop interactive dashboards for real-time exploration. Add plain-language explanations for all technical terms. Create video presentations for complex concepts. |
| **13. Productization** | Designed modular code structure with separate functions for data processing, simulation, and reporting. Created checkpoint system for recovery from failures. Documented all parameters and configuration options. | Code initially lacked proper error handling and logging. Reproducibility was challenging without proper version control. Performance optimization needed for production scale. | Implemented comprehensive logging and checkpoint strategy. Added input validation and error handling. Created configuration files for easy parameter updates. | Would containerize the application using Docker. Implement proper CI/CD pipeline. Add automated testing suite for all components. |
| **14. Deployment & Monitoring** | Planned deployment with daily data updates and weekly portfolio refresh. Designed monitoring for data quality, model performance, and system health. Created alerting for drift in key metrics. | Defining appropriate thresholds for model drift was challenging. Balancing automation with human oversight required careful planning. Real-time monitoring infrastructure needs were complex. | Set conservative thresholds initially with plan to tune based on experience. Created four-layer monitoring (Data, Model, System, Business). Implemented escalation procedures for different severity levels. | Would implement A/B testing framework for model improvements. Add automated model retraining capabilities. Create comprehensive dashboard for monitoring all aspects. |
| **15. Orchestration & System Design** | Designed 6-task pipeline (ingest → clean → metrics → simulate → rank → report) with clear dependencies. Created automation strategy balancing efficiency with risk management. Implemented checkpoint recovery system. | Deciding what to automate immediately vs keep manual was challenging. Task dependencies created single points of failure. Resource management for large simulations needed optimization. | Automated low-risk data operations (first 3 tasks), kept investment decisions manual. Created checkpoint strategy for recovery. Planned gradual automation expansion over 6-12 months. | Would implement parallel processing for portfolio simulations. Add dynamic resource scaling. Create more sophisticated workflow orchestration (Airflow). |
| **16. Lifecycle Review & Reflection** | This project successfully demonstrated end-to-end quantitative finance pipeline from problem definition to production-ready solution. Learned importance of balancing academic rigor with practical constraints. Gained experience in stakeholder communication and risk management. | Data quality management was more complex than anticipated. Behavioral modeling (retail investor simulation) required iterative refinement. Production considerations were more extensive than expected. | Systematic approach to each lifecycle stage prevented major issues. Early stakeholder engagement helped shape realistic requirements. Documentation and checkpointing enabled recovery from setbacks. | Would start with simpler scope and iterate more frequently. Invest more time upfront in data infrastructure. Build stakeholder education into the project timeline from the beginning. |

---

## Reflection Prompts

**Which stage was the most difficult for you, and why?**  
Data Preprocessing (Stage 6) was most challenging because of the complexity of handling 14 years of financial data with corporate actions, delistings, and missing values. Distinguishing between data quality issues and legitimate market events required significant domain knowledge and validation effort.

**Which stage was the most rewarding?**  
Results Reporting and Communication (Stage 12) was most rewarding because it brought together all the technical work into actionable insights. Seeing stakeholders understand and appreciate the top 10 stock recommendations made the entire effort worthwhile.

**How do the stages connect — where did one stage's decisions constrain or enable later stages?**  
Problem framing decisions about retail investor behavior directly shaped the portfolio simulation constraints in Stage 10. Data quality choices in preprocessing determined which stocks could be included in the final analysis. The checkpoint strategy from Stage 13 enabled robust orchestration in Stage 15.

**If you repeated this project, what would you do differently across the lifecycle?**  
I would invest more upfront time in data infrastructure (Stages 4-5) and stakeholder education about quantitative methods. Starting with a smaller proof-of-concept would allow for faster iteration and validation of key assumptions before scaling to the full 14-year dataset.

**Which skills do you most want to strengthen before your next financial engineering project?**  
Advanced time-series analysis, factor modeling, and production system design (Docker, CI/CD, monitoring). Also want to improve stakeholder communication skills for explaining complex quantitative concepts to business audiences.